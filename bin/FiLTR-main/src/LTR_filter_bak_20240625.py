import argparse
import os
import subprocess
import sys
import time



current_folder = os.path.dirname(os.path.abspath(__file__))
# Add the path to the 'configs' folder to the Python path
configs_folder = os.path.join(current_folder, "..")
sys.path.append(configs_folder)

from multiprocessing import cpu_count
from Util import read_fasta, store_fasta, read_fasta_v1, flank_region_align_v5, Logger, read_scn, store_scn, \
    get_LTR_seq_from_scn, get_recombination_ltr, deredundant_for_LTR, generate_both_ends_frame_from_seq, \
    filter_tir_by_tsd, get_low_copy_LTR, judge_ltr_from_both_ends_frame, judge_ltr_has_structure, file_exist, \
    get_high_copy_LTR, rename_reference, rename_fasta, alter_deep_learning_results, get_full_length_copies_batch, \
    filter_tir, judge_ltr_from_both_ends_frame_v1, filter_ltr_by_structure
from configs import config
from utils.data_util import expand_matrix_dir






if __name__ == '__main__':
    tool_name = 'Ltr_filter'
    version_num = '0.0.1'
    default_threads = int(cpu_count())
    default_miu = str(1.3e-8)
    default_flanking_len = 50

    default_is_remove_recomb = 1
    default_is_use_flank_MSA = 1
    default_is_filter_tandem = 1
    default_is_filter_TIR = 1
    default_is_handle_low_copy = 1
    default_is_use_deep_model = 1
    default_is_use_structure = 1
    default_is_clean_internal = 1
    default_is_remove_nested = 1
    default_is_deredundant = 1
    default_recover = 0

    default_skip_detect = 0
    default_debug = 1
    default_BM_RM2 = 0
    default_BM_EDTA = 0
    default_EDTA_home = ''
    default_BM_HiTE = 0
    default_coverage_threshold = 0.95


    # 1.parse args
    describe_info = '########################## ' + tool_name + ', version ' + str(version_num) + ' ##########################'
    parser = argparse.ArgumentParser(description=describe_info)
    parser.add_argument('--scn', required=True, metavar='scn', help='Input scn file generated by LTR_finder and LTR_harvest')
    parser.add_argument('--genome', required=True, metavar='genome', help='Input genome assembly path')
    parser.add_argument('--out_dir', required=True, metavar='output_dir',
                        help='The path of output directory; It is recommended to use a new directory to avoid automatic deletion of important files.')

    parser.add_argument('--thread', metavar='thread_num',
                        help='Input thread num, default = [ ' + str(default_threads) + ' ]')
    parser.add_argument('--miu', metavar='miu',
                        help='The neutral mutation rate (per bp per ya), default = [ ' + str(default_miu) + ' ]')
    parser.add_argument('--flanking_len', metavar='flanking_len',
                        help='The flanking length of candidates to find the true boundaries, default = [ ' + str(default_flanking_len) + ' ]')
    parser.add_argument('--skip_detect', metavar='skip_detect',
                        help='Whether to skip_HiTE, 1: true, 0: false. default = [ ' + str(default_skip_detect) + ' ]')
    parser.add_argument('--debug', metavar='is_debug',
                        help='Open debug mode, and temporary files will be kept, 1: true, 0: false. default = [ ' + str(default_debug) + ' ]')

    parser.add_argument('--is_remove_recomb', metavar='is_remove_recomb',
                        help='Whether to filter recombined LTRs. default = [ ' + str(default_is_remove_recomb) + ' ]')
    parser.add_argument('--is_use_flank_MSA', metavar='is_use_flank_MSA',
                        help='Whether to use the flanking region multiple sequence alignment strategy for filtering. default = [ ' + str(default_is_use_flank_MSA) + ' ]')
    parser.add_argument('--is_filter_tandem', metavar='is_filter_tandem',
                        help='Whether to filter LTR terminals composed of tandem repeats. default = [ ' + str(default_is_filter_tandem) + ' ]')
    parser.add_argument('--is_filter_TIR', metavar='is_filter_TIR',
                        help='Whether to filter LTR terminals composed of terminal inverted repeats (TIR). default = [ ' + str(default_is_filter_TIR) + ' ]')
    parser.add_argument('--is_handle_low_copy', metavar='is_handle_low_copy',
                        help='Whether to handle low-copy LTR separately. default = [ ' + str(default_is_handle_low_copy) + ' ]')
    parser.add_argument('--is_use_deep_model', metavar='is_use_deep_model',
                        help='Whether to use deep learning model. default = [ ' + str(default_is_use_deep_model) + ' ]')
    parser.add_argument('--is_use_structure', metavar='is_use_structure',
                        help='Whether to use deep learning model. default = [ ' + str(default_is_use_structure) + ' ]')
    parser.add_argument('--is_clean_internal', metavar='is_clean_internal',
                        help='Whether to clean LTR internal sequences. default = [ ' + str(default_is_clean_internal) + ' ]')
    parser.add_argument('--is_remove_nested', metavar='is_remove_nested',
                        help='Whether to unpack nested LTRs. default = [ ' + str(default_is_remove_nested) + ' ]')
    parser.add_argument('--is_deredundant', metavar='is_deredundant',
                        help='Whether to generate LTR cons. default = [ ' + str(default_is_deredundant) + ' ]')
    parser.add_argument('--recover', metavar='is_recover',
                        help='Whether to enable recovery mode to avoid starting from the beginning, 1: true, 0: false. default = [ ' + str(default_recover) + ' ]')

    parser.add_argument('--BM_RM2', metavar='BM_RM2',
                        help='Whether to conduct benchmarking of RepeatModeler2, 1: true, 0: false. default = [ ' + str(default_BM_RM2) + ' ]')
    parser.add_argument('--BM_EDTA', metavar='BM_EDTA',
                        help='Whether to conduct benchmarking of EDTA, 1: true, 0: false. default = [ ' + str(default_BM_EDTA) + ' ]')
    parser.add_argument('--BM_HiTE', metavar='BM_HiTE',
                        help='Whether to conduct benchmarking of HiTE, 1: true, 0: false. default = [ ' + str(default_BM_HiTE) + ' ]')
    parser.add_argument('--EDTA_home', metavar='EDTA_home',
                        help='When conducting benchmarking of EDTA, you will be asked to input EDTA home path.')
    parser.add_argument('--coverage_threshold', metavar='coverage_threshold',
                        help='The coverage threshold of benchmarking methods. default = [ ' + str(default_coverage_threshold) + ' ]')
    parser.add_argument('--species', metavar='species',
                        help='Which species you want to conduct benchmarking, six species support (dmel, rice, cb, zebrafish, maize, ath).')

    args = parser.parse_args()

    scn_file = args.scn
    reference = args.genome
    output_dir = args.out_dir
    threads = args.thread
    miu = args.miu
    flanking_len = args.flanking_len
    skip_detect = args.skip_detect
    debug = args.debug

    is_remove_recomb = args.is_remove_recomb
    is_use_flank_MSA = args.is_use_flank_MSA
    is_filter_tandem = args.is_filter_tandem
    is_filter_TIR = args.is_filter_TIR
    is_handle_low_copy = args.is_handle_low_copy
    is_use_deep_model = args.is_use_deep_model
    is_use_structure = args.is_use_structure
    is_clean_internal = args.is_clean_internal
    is_remove_nested = args.is_remove_nested
    is_deredundant = args.is_deredundant
    recover = args.recover

    BM_RM2 = args.BM_RM2
    BM_EDTA = args.BM_EDTA
    BM_HiTE = args.BM_HiTE
    EDTA_home = args.EDTA_home
    coverage_threshold = args.coverage_threshold
    species = args.species

    tmp_output_dir = os.path.abspath(output_dir + '/')
    if not os.path.exists(tmp_output_dir):
        os.makedirs(tmp_output_dir)

    project_dir = config.project_dir
    src_dir = project_dir + '/src'
    tool_dir = project_dir + '/tools'

    log = Logger(tmp_output_dir + '/LtrHomo.log', level='debug')

    if reference is None:
        log.logger.error('\nGenome path can not be empty')
        parser.print_help()
        exit(-1)
    if output_dir is None:
        output_dir = project_dir + '/output'
        log.logger.warning('\noutput directory path is empty, set to: ' + str(output_dir))

    if not os.path.isabs(scn_file):
        scn_file = os.path.abspath(scn_file)
    if not os.path.isabs(reference):
        reference = os.path.abspath(reference)
    if not os.path.isabs(output_dir):
        output_dir = os.path.abspath(output_dir)

    if threads is None:
        threads = int(default_threads)
    else:
        threads = int(threads)

    if flanking_len is None:
        flanking_len = default_flanking_len
    else:
        flanking_len = int(flanking_len)

    if miu is None:
        miu = default_miu
    else:
        miu = str(miu)

    if is_remove_recomb is None:
        is_remove_recomb = default_is_remove_recomb
    else:
        is_remove_recomb = int(is_remove_recomb)

    if is_use_flank_MSA is None:
        is_use_flank_MSA = default_is_use_flank_MSA
    else:
        is_use_flank_MSA = int(is_use_flank_MSA)

    if is_filter_tandem is None:
        is_filter_tandem = default_is_filter_tandem
    else:
        is_filter_tandem = int(is_filter_tandem)

    if is_filter_TIR is None:
        is_filter_TIR = default_is_filter_TIR
    else:
        is_filter_TIR = int(is_filter_TIR)

    if is_handle_low_copy is None:
        is_handle_low_copy = default_is_handle_low_copy
    else:
        is_handle_low_copy = int(is_handle_low_copy)

    if is_use_deep_model is None:
        is_use_deep_model = default_is_use_deep_model
    else:
        is_use_deep_model = int(is_use_deep_model)

    if is_use_structure is None:
        is_use_structure = default_is_use_structure
    else:
        is_use_structure = int(is_use_structure)

    if is_clean_internal is None:
        is_clean_internal = default_is_clean_internal
    else:
        is_clean_internal = int(is_clean_internal)

    if is_remove_nested is None:
        is_remove_nested = default_is_remove_nested
    else:
        is_remove_nested = int(is_remove_nested)

    if is_deredundant is None:
        is_deredundant = default_is_deredundant
    else:
        is_deredundant = int(is_deredundant)

    if recover is None:
        recover = default_recover
    else:
        recover = int(recover)

    if BM_RM2 is None:
        BM_RM2 = default_BM_RM2
    else:
        BM_RM2 = int(BM_RM2)

    if BM_EDTA is None:
        BM_EDTA = default_BM_EDTA
    else:
        BM_EDTA = int(BM_EDTA)

    if BM_HiTE is None:
        BM_HiTE = default_BM_HiTE
    else:
        BM_HiTE = int(BM_HiTE)

    if EDTA_home is None:
        EDTA_home = default_EDTA_home
    else:
        EDTA_home = str(EDTA_home)

    if coverage_threshold is None:
        coverage_threshold = default_coverage_threshold
    else:
        coverage_threshold = float(coverage_threshold)

    if skip_detect is None:
        skip_HiTE = default_skip_detect
    else:
        skip_HiTE = int(skip_detect)

    if debug is None:
        debug = default_debug
    else:
        debug = int(debug)


    log.logger.info('Important Note: Please ensure that the chromosome names in the genome are in the format of Chr+number, such as Chr1 and Chr2.')

    # Step1: Filter records where the internal sequence of the current LTR contains its terminal sequences. This situation arises due to recombination involving shared terminals of two or more LTRs.
    ref_names, ref_contigs = read_fasta(reference)
    ltr_candidates, ltr_lines = read_scn(scn_file, log, remove_dup=True)

    if is_remove_recomb:
        remove_recomb_scn = tmp_output_dir + '/remove_recomb.scn'
        result_file = remove_recomb_scn
        if not recover or not file_exist(result_file):
            log.logger.info('Start step1: Filter records where the internal sequence of the current LTR contains its terminal sequences. This situation arises due to recombination involving shared terminals of two or more LTRs.')
            recombination_candidates = get_recombination_ltr(ltr_candidates, ref_contigs, threads, log)
            confident_lines = []
            for candidate_index in ltr_candidates.keys():
                if candidate_index not in recombination_candidates:
                    line = ltr_lines[candidate_index]
                    confident_lines.append(line)
            store_scn(confident_lines, remove_recomb_scn)
            log.logger.debug('Remove recombination LTR: ' + str(len(recombination_candidates)) + ', remaining LTR num: ' + str(len(confident_lines)))
        else:
            log.logger.info(result_file + ' exists, skip...')
        scn_file = remove_recomb_scn


    split_ref_dir = tmp_output_dir + '/ref_chr'
    test_home = src_dir
    result_file = split_ref_dir
    if not recover or not os.path.exists(result_file):
        split_genome_command = 'cd ' + test_home + ' && python3 ' + test_home + '/split_genome_chunks.py -g ' \
                               + reference + ' --tmp_output_dir ' + tmp_output_dir
        os.system(split_genome_command)
    else:
        log.logger.info(result_file + ' exists, skip...')

    # Based on the SCN file, extract the left LTR terminal sequences and create a mapping between the LTR sequences and the SCN lines.
    # This will facilitate the subsequent filtering of the SCN file.
    left_ltr_path = tmp_output_dir + '/left_LTR.fa'
    ltr_candidates, ltr_lines = read_scn(scn_file, log)
    leftLtr2Candidates = {}
    left_LTR_contigs = {}
    for candidate_index in ltr_candidates.keys():
        (chr_name, left_ltr_start, left_ltr_end, right_ltr_start, right_ltr_end) = ltr_candidates[candidate_index]
        if chr_name not in ref_contigs:
            log.logger.error('Error: Chromosome names in the SCN file do not match the input genome names. Please correct this and rerun.')
            exit(-1)
        ref_seq = ref_contigs[chr_name]
        left_ltr_name = chr_name + ':' + str(left_ltr_start) + '-' + str(left_ltr_end)
        left_ltr_seq = ref_seq[left_ltr_start-1: left_ltr_end]
        # process duplicate name
        while left_ltr_name in left_LTR_contigs:
            left_ltr_name += '-replicate'
        left_LTR_contigs[left_ltr_name] = left_ltr_seq
        leftLtr2Candidates[left_ltr_name] = candidate_index
    store_fasta(left_LTR_contigs, left_ltr_path)

    if is_filter_tandem:
        left_ltr_filter_path = left_ltr_path + '.filter_tandem'
        result_file = left_ltr_filter_path
        if not recover or not file_exist(result_file):
            # Step 2: Filter out LTR termini primarily composed of tandem repeats.
            log.logger.info('Step 2: Filter out LTR termini primarily composed of tandem repeats.')
            tandem_filter_command = 'python ' + src_dir + '/filter_tandem_repeats.py -f ' + left_ltr_path + ' > ' + left_ltr_filter_path
            os.system(tandem_filter_command)
        else:
            log.logger.info(result_file + ' exists, skip...')
        left_ltr_path = left_ltr_filter_path
        filter_left_ltr_names, filter_left_ltr_contigs = read_fasta(left_ltr_path)
        log.logger.debug('Remove tandem LTR: ' + str(len(left_LTR_contigs) - len(filter_left_ltr_contigs)) + ', remaining LTR num: ' + str(len(filter_left_ltr_contigs)))

    if is_use_flank_MSA:
        # Step 3: Filter out false positive sequences based on the flanking regions of the terminal sequences.
        log.logger.info('Step 3: Filter out false positive sequences based on the flanking regions of the terminal sequences.')

        confident_scn = tmp_output_dir + '/confident_ltr.scn'
        result_file = confident_scn
        if not recover or not file_exist(result_file):
            if os.path.exists(result_file):
                os.system('rm -f ' + result_file)

            # Perform a multiple sequence alignment of the regions flanking the LTR terminal sequence copies.
            temp_dir = tmp_output_dir + '/candidate_ltr'
            output_dir = tmp_output_dir + '/ltr_both_frames'
            result_file = output_dir
            if not recover or not os.path.exists(result_file):
                log.logger.debug('Generate LTR frames')
                generate_both_ends_frame_from_seq(left_ltr_path, reference, threads, temp_dir, output_dir, split_ref_dir)
            else:
                log.logger.info(result_file + ' exists, skip...')


            output_path = tmp_output_dir + '/is_LTR.txt'
            if is_handle_low_copy:
                structure_output_path = tmp_output_dir + '/is_LTR_structure.lc.txt'
                result_file = structure_output_path
                if not recover or not file_exist(result_file):
                    # Step 3.1: Use a rule-based method to filter out low-copy LTRs.
                    log.logger.debug('Step 3.1: Use a rule-based method to filter out low-copy LTRs.')
                    low_copy_output_dir = tmp_output_dir + '/low_copy_frames'
                    get_low_copy_LTR(output_dir, low_copy_output_dir, copy_num_threshold=5)

                    type = 'Low copy'
                    lc_output_path = tmp_output_dir + '/is_LTR_homo.lc.txt'
                    judge_ltr_from_both_ends_frame(low_copy_output_dir, lc_output_path, threads, type, log)

                    # Step 3.2: Determine if the low-copy LTR possesses a 4-6 bp TSD.
                    log.logger.debug('Step 3.2: Determine if the low-copy LTR possesses a 4-6 bp TSD.')
                    judge_ltr_has_structure(lc_output_path, structure_output_path, leftLtr2Candidates, ltr_lines, reference, log)
                else:
                    log.logger.info(result_file + ' exists, skip...')
                os.system('cat ' + structure_output_path + ' >> ' + output_path)

            if is_use_deep_model:
                alter_dl_output_path = tmp_output_dir + '/is_LTR_deep.alter.txt'
                result_file = alter_dl_output_path
                if not recover or not file_exist(result_file):
                    # Step 3.3: 基于深度学习方法判断
                    # Step4. 调用深度学习模型预测 左侧框是否LTR
                    # 调用深度学习模型之前，先将窗口进行扩展
                    log.logger.debug('Copy high-copy LTR frames for deep learning predicting')
                    high_copy_output_dir = tmp_output_dir + '/high_copy_frames'
                    get_high_copy_LTR(output_dir, high_copy_output_dir, copy_num_threshold=5)

                    # 调用规则的同源方法过滤
                    hc_output_path = tmp_output_dir + '/is_LTR_homo.hc.txt'
                    type = 'High copy'
                    judge_ltr_from_both_ends_frame(high_copy_output_dir, hc_output_path, threads, type, log)
                    # judge_ltr_from_both_ends_frame_v1(high_copy_output_dir, hc_output_path, left_LTR_contigs, tmp_output_dir, threads, type, log)

                    dl_output_path = tmp_output_dir + '/is_LTR_deep.txt'
                    # model_path = project_dir + '/models/model_30_0.0001_0.005_32.pth'
                    # classify_command = 'python ' + src_dir + '/Deep_Learning/myclassifier_cat3.py --data_dir ' + high_copy_output_dir + \
                    #                    ' --out_dir ' + tmp_output_dir + ' --model_path ' + model_path + \
                    #                    ' --threads ' + str(threads)
                    # log.logger.debug(classify_command)
                    # os.system(classify_command)
                    model_path = project_dir + '/models/model_30_0.0001_0.005_32_0.9467945494283563.pth'
                    classify_command = 'python ' + src_dir + '/Deep_Learning/myclassifier_neuralLTR.py --data_dir ' + high_copy_output_dir + \
                                       ' --out_dir ' + tmp_output_dir + ' --model_path ' + model_path + \
                                       ' --threads ' + str(threads)
                    log.logger.debug(classify_command)
                    os.system(classify_command)

                    # model_path = project_dir + '/models/model_fold_0.h5'
                    # classify_command = 'python ' + src_dir + '/Deep_Learning/Classifier.py --data_dir ' + high_copy_output_dir + \
                    #                    ' --out_dir ' + tmp_output_dir + ' --model_path ' + model_path + \
                    #                    ' --threads ' + str(threads)
                    # log.logger.debug(classify_command)
                    # os.system(classify_command)

                    # 使用同源规则的结果对深度学习的预测结果进行调整
                    alter_deep_learning_results(dl_output_path, hc_output_path, alter_dl_output_path)
                else:
                    log.logger.info(result_file + ' exists, skip...')
                # 合并 高 拷贝预测结果
                os.system('cat ' + alter_dl_output_path + ' >> ' + output_path)

            if is_filter_TIR:
                tir_output_path = tmp_output_dir + '/is_LTR_tir.txt'
                result_file = tir_output_path
                if not recover or not file_exist(result_file):
                    # Step 4.2 识别窗口两侧是否具有 TIR TSD特征，如果有超过10个拷贝有TSD特征，则认为是假阳性
                    filter_tir(output_path, tir_output_path, output_dir, threads, left_LTR_contigs, tmp_output_dir, tool_dir, log)
                else:
                    log.logger.info(result_file + ' exists, skip...')
                output_path = tir_output_path


            # 我们要求完整的LTR必须具备 4-6 bp TSD，没有的话就认为是假阳性
            # 由于初始的边界不一定准确，我们按照LTR_retriever的方法，取侧翼 8bp 和 5' end 3bp，共11bp，然后搜索4-6bp TSD，如果没有，再搜索TG CA，如果都没有就当作假阳性过滤
            if is_use_structure:
                structure_output_path = tmp_output_dir + '/is_LTR_structure.txt'
                result_file = structure_output_path
                if not recover or not file_exist(result_file):
                    filter_ltr_by_structure(output_path, structure_output_path, leftLtr2Candidates, ltr_lines, reference, threads, log)
                else:
                    log.logger.info(result_file + ' exists, skip...')
                output_path = structure_output_path

            # Step5. 过滤掉False positives
            FP_ltrs = {}
            true_ltrs = {}
            with open(output_path, 'r') as f_r:
                for line in f_r:
                    line = line.replace('\n', '')
                    parts = line.split('\t')
                    ltr_name = parts[0]
                    is_ltr = int(parts[1])
                    if not is_ltr:
                        FP_ltrs[ltr_name] = is_ltr
                    else:
                        true_ltrs[ltr_name] = is_ltr

            confident_lines = []
            for name in leftLtr2Candidates.keys():
                # if name not in FP_ltrs:
                if name in true_ltrs:
                    candidate_index = leftLtr2Candidates[name]
                    line = ltr_lines[candidate_index]
                    confident_lines.append(line)
            store_scn(confident_lines, confident_scn)
        else:
            log.logger.info(result_file + ' exists, skip...')
        scn_file = confident_scn

    confident_ltr_path = tmp_output_dir + '/confident_ltr.fa'
    result_file = confident_ltr_path
    if not recover or not file_exist(result_file):
        # Step6. 生成LTR library
        confident_ltr_terminal = tmp_output_dir + '/confident_ltr.terminal.fa'
        confident_ltr_internal = tmp_output_dir + '/confident_ltr.internal.fa'
        get_LTR_seq_from_scn(reference, scn_file, confident_ltr_terminal, confident_ltr_internal)

        confident_ltr_terminal_cons = confident_ltr_terminal + '.cons'
        if is_deredundant:
            # Step7. Remove redundancy from the LTR terminal results.
            starttime = time.time()
            log.logger.info('Start step6: Remove LTR terminal redundancy')
            deredundant_for_LTR(confident_ltr_terminal, tmp_output_dir, threads)
            endtime = time.time()
            dtime = endtime - starttime
            log.logger.info("Running time of step6: %.8s s" % (dtime))
        else:
            cd_hit_command = 'cd-hit-est -aS ' + str(0.95) + ' -aL ' + str(0.95) + ' -c ' + str(0.8) \
                             + ' -G 0 -g 1 -A 80 -i ' + confident_ltr_terminal + ' -o ' + confident_ltr_terminal_cons + ' -T 0 -M 0'
            os.system(cd_hit_command + ' > /dev/null 2>&1')
        # rename_fasta(confident_ltr_terminal_cons, confident_ltr_terminal_cons, 'LTR_terminal')

        if is_clean_internal:
            # Step6.2 清理内部序列。大量的内部序列是由于其他类型的TE插入，或者干脆内部序列就是大量串联重复，我们需要排除这种影响。
            log.logger.info('Start step7: Clean the internal sequences of LTRs by removing tandem repeats, LINEs, TIRs, and other elements (using proteins).')
            clean_internal_command = 'python3 /home/hukang/NeuralLTR/src/clean_LTR_internal.py ' \
                                    + ' -t ' + str(threads) \
                                    + ' --tmp_output_dir ' + tmp_output_dir \
                                    + ' --internal_seq ' + confident_ltr_internal
            log.logger.debug(clean_internal_command)
            os.system(clean_internal_command)
            confident_ltr_internal += '.clean'

        if is_remove_nested:
            # Step6.1 去内部嵌合 LTR 和 Helitron元素。
            log.logger.info('Start step8: Remove nested LTR and Helitron elements within LTR sequences.')
            clean_LTR_path = confident_ltr_internal + '.clean_nested'
            confident_ltr_tmp = tmp_output_dir + '/confident_ltr.tmp.fa'
            # helitron_lib = project_dir + '/databases/all_helitron.ref'
            max_iter_num = 3
            for i in range(max_iter_num):
                os.system('cat ' + confident_ltr_terminal_cons + ' ' + confident_ltr_internal + ' > ' + confident_ltr_tmp)
                # os.system('cat ' + confident_ltr_terminal_cons + ' ' + confident_ltr_internal + ' ' + helitron_lib + ' > ' + confident_ltr_tmp)
                remove_nested_command = 'python3 /home/hukang/NeuralLTR/src/remove_nested_lib.py ' \
                                        + ' -t ' + str(threads) \
                                        + ' --tmp_output_dir ' + tmp_output_dir + ' --max_iter_num ' + str(1) \
                                        + ' --input1 ' + confident_ltr_tmp \
                                        + ' --input2 ' + confident_ltr_internal \
                                        + ' --output ' + clean_LTR_path
                log.logger.debug(remove_nested_command)
                os.system(remove_nested_command)
                confident_ltr_internal = clean_LTR_path

        # 判断内部序列是否在整个基因组上存在至少一个以上的全长拷贝
        temp_dir = tmp_output_dir + '/internal_blast_temp'
        all_copies = get_full_length_copies_batch(confident_ltr_internal, split_ref_dir, threads, temp_dir)
        confident_internal_names, confident_internal_contigs = read_fasta(confident_ltr_internal)
        confident_ltr_internal += '.confident'
        for name in all_copies.keys():
            if len(all_copies[name]) < 1:
                del confident_internal_contigs[name]
        store_fasta(confident_internal_contigs, confident_ltr_internal)


        confident_ltr_internal_cons = confident_ltr_internal + '.cons'
        if is_deredundant:
            # Step7. Remove redundancy from the LTR internal results.
            starttime = time.time()
            log.logger.info('Start step6: Remove LTR redundancy')
            deredundant_for_LTR(confident_ltr_internal, tmp_output_dir, threads)
            endtime = time.time()
            dtime = endtime - starttime
            log.logger.info("Running time of step6: %.8s s" % (dtime))
        else:
            cd_hit_command = 'cd-hit-est -aS ' + str(0.95) + ' -aL ' + str(0.95) + ' -c ' + str(0.8) \
                             + ' -G 0 -g 1 -A 80 -i ' + confident_ltr_internal + ' -o ' + confident_ltr_internal_cons + ' -T 0 -M 0'
            os.system(cd_hit_command + ' > /dev/null 2>&1')
        # rename_fasta(confident_ltr_internal_cons, confident_ltr_internal_cons, 'LTR_internal')

        # Step8. 生成一致性library
        os.system('cat ' + confident_ltr_terminal_cons + ' ' + confident_ltr_internal_cons + ' > ' + confident_ltr_path)
    else:
        log.logger.info(result_file + ' exists, skip...')

    # Step9. 调用评估方法
    evaluation_command = 'cd ' + project_dir + ' && python ' + src_dir + '/benchmarking.py --BM_RM2 ' + str(BM_RM2) + ' --BM_EDTA ' + str(BM_EDTA) + ' --BM_HiTE ' + str(BM_HiTE) + ' -t ' + \
                      str(threads) + ' --TE_lib ' + confident_ltr_path + ' -r ' + reference + \
                      ' --EDTA_home ' + str(EDTA_home) + ' --species ' + species + ' --tmp_output_dir ' + tmp_output_dir + ' --recover ' + str(recover)
    log.logger.debug(evaluation_command)
    os.system(evaluation_command)

